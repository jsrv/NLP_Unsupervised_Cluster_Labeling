{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Brigham Young University\n",
    "    Bailey Smith\n",
    "    Juan S. Rodriguez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import re\n",
    "import glob\n",
    "import time\n",
    "import codecs\n",
    "import pickle\n",
    "import wikipedia\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "#import visualizations\n",
    "%matplotlib inline\n",
    "\n",
    "from tqdm import tqdm\n",
    "from scipy.spatial import KDTree\n",
    "from collections import Counter\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score\n",
    "from xgboost import XGBClassifier, XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Maplotlib customizations.\n",
    "plt.style.use(\"seaborn\")\n",
    "plt.rcParams[\"figure.figsize\"] = [5, 3] # Set the figure size.\n",
    "plt.rcParams[\"figure.dpi\"] = 200 # Raise figure quality.\n",
    "\n",
    "# Pandas customizations\n",
    "pd.set_option(\"display.max_rows\", 5) # Number of rows displayed.\n",
    "pd.set_option(\"display.max_columns\", 5) # Number of columns displayed.\n",
    "pd.set_option(\"precision\", 3) # Truncate floats to 3 decimals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "## Abstract\n",
    "TED talks have become a standard on how to deliver informational talks. All of these talks are presented in a digital format online for public viweing. By using the transcripts and website data for all TED talks given up to September 2017 +++. Each video in the website is provided with suggested similar videos and informative topic-tags on them. We present an improvement in the suggested videos, and we consolidate a base on tags that are more useful by using bisecting k-means and LDA-GS respectively.\n",
    "\n",
    "## TED Talks\n",
    "Since 1984, TED has become an iconic conference in which experts of the world in different fields present their ideas and analysis in the fields of technology, entertainment and design (T.E.D.). Since 2006, the conference platform decided to make every talk public and free by publishing them on their website. Given its history and prestige, TED talks have become a standard for quality when it comes to delivering an informational talk to an audience.\n",
    "\n",
    "## Motivation\n",
    "Our purpose is to analyze every talk that has been published online up to September 2017 in order to suggest improved methods to classify them. We will create a method that allows automatic classification/labelling of the TED talks just by reading in the transcript. \n",
    "\n",
    "An automatic classification method would allow better suggestions to the online audience, would discard the need of **biased-human classification**, and would be generalizable to other fields and problems. This sort of classification method would allow a computer to sort through thousands of legal documents, health documents, speech transcript, or even books, and allow the scholar to approach them seamlessly. \n",
    "\n",
    "## Literary Review\n",
    "\n",
    "We based the clustering techniques on some papers:\n",
    "\n",
    "A Comparison of Document Clustering Techniques - Michael Steinbach\n",
    "\n",
    "http://mlwiki.org/index.php/Latent_Semantic_Analysis#Problems_with_Text\n",
    "\n",
    "http://mlwiki.org/index.php/Document_Clustering#Semi-Supervised_Clustering\n",
    "\n",
    "## Approach (AKA: to-do):\n",
    "For tagging: we use the LDA algorithm with Gibbs Sampling and provide 21 different labels for the videos to have. We generate a clustering based on a K-means bisection in order to determine the main clusters of videos based on their transcripts. We attempt several different algorithms, or a ensemble of them to get the best possible clustering. We predict the audience interested in the talk based on the cluster they have watched.\n",
    "\n",
    "We suggest a new network of related videos by computing the highest similarity (PCA and TFIDF) and clustering the given video with those in that cluster or in any cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Upload cluster words\n",
    "A21 = pd.read_csv('WordSets/df_NLP_21a.csv',header=None).as_matrix()\n",
    "B21 = pd.read_csv('WordSets/df_NLP_21b.csv',header=None).as_matrix()\n",
    "C21 = pd.read_csv('WordSets/df_NLP_21c.csv',header=None).as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kids' 'school' 'children' 'students' 'education' 'teachers' 'learning'\n",
      " 'schools' 'child' 'teacher']\n",
      "[nan 'people' 'time' 'life' 'years' 'day' 'back' 'world' 'thing' 'make']\n",
      "['music' 'play' 'sound' 'game' 'games' 'song' 'video' 'playing' 'hear'\n",
      " 'sounds']\n",
      "['water' 'energy' 'earth' 'planet' 'climate' 'carbon' 'years' 'oil' 'air'\n",
      " 'mars']\n",
      "['data' 'internet' 'information' 'online' 'media' 'phone' 'digital'\n",
      " 'people' 'google' 'web']\n",
      "['virus' 'hiv' 'disease' 'flu' 'malaria' 'vaccine' 'polio' 'infected'\n",
      " 'epidemic' 'vaccines']\n",
      "['people' 'human' 'social' 'life' 'love' 'good' 'happiness' 'compassion'\n",
      " 'feel' 'god']\n",
      "['language' 'books' 'laughter' 'english' 'book' 'words' 'word' 'read'\n",
      " 'stories' 'film']\n",
      "['ocean' 'fish' 'sea' 'animals' 'water' 'species' 'boat' 'coral' 'oceans'\n",
      " 'ice']\n",
      "['universe' 'light' 'space' 'stars' 'theory' 'physics' 'earth' 'planets'\n",
      " 'particles' 'black']\n",
      "['city' 'cities' 'car' 'cars' 'urban' 'street' 'york' 'public' 'map'\n",
      " 'streets']\n",
      "['bees' 'poem' 'poetry' 'bee' 'soap' 'poems' 'pollen' 'paper' 'flowers'\n",
      " 'flower']\n",
      "['people' 'world' 'percent' 'countries' 'money' 'country' 'years' 'dollars'\n",
      " 'africa' 'global']\n",
      "['cancer' 'health' 'patients' 'cells' 'disease' 'medical' 'blood' 'care'\n",
      " 'patient' 'heart']\n",
      "['brain' 'robot' 'robots' 'body' 'neurons' 'brains' 'memory'\n",
      " 'consciousness' 'cells' 'arm']\n",
      "['war' 'people' 'political' 'power' 'country' 'democracy' 'violence'\n",
      " 'states' 'police' 'rights']\n",
      "['dna' 'species' 'animals' 'human' 'genes' 'gene' 'bacteria' 'cells' 'cell'\n",
      " 'humans']\n",
      "['food' 'eat' 'farmers' 'plant' 'feed' 'bread' 'agriculture' 'eating'\n",
      " 'meat' 'farm']\n",
      "['design' 'art' 'building' 'architecture' 'project' 'space' 'buildings'\n",
      " 'designers' 'museum' 'made']\n",
      "['women' 'men' 'girls' 'woman' 'sex' 'black' 'gender' 'girl' 'gay' 'female']\n",
      "[nan 'people' 'things' 'time' 'make' 'thing' 'world' 'years' 'kind' 'lot']\n"
     ]
    }
   ],
   "source": [
    "print(*A21,sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A farmer (also called an agriculturer) is a person engaged in agriculture, raising living organisms for food or raw materials. The term usually applies to people who do some combination of raising field crops, orchards, vineyards, poultry, or other livestock. A farmer might own the farmed land or might work as a laborer on land owned by others, but in advanced economies, a farmer is usually a farm owner, while employees of the farm are known as farm workers, or farmhands. However, in the not so distant past, a farmer was a person who promotes or improves the growth of (a plant, crop, etc.) by labor and attention, land or crops or raises animals (as livestock or fish).'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikipedia.summary('farmers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mainData = pd.read_csv('ted_main.csv')\n",
    "transcripts = pd.read_csv('transcripts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TED Talks Data\n",
    "## Data Description\n",
    "### Main Dataset\n",
    "The main dataset contains descriptive information for 2550 TED Talks.\n",
    "\n",
    "    title:            (str)     The title of the talk\n",
    "    description:      (str)     A blurb of what the talk is about\n",
    "    main_speaker:     (str)     The first named speaker of the talk\n",
    "    speaker_occ:      (str)     The occupation of the main speaker\n",
    "    num_speaker:      (int)     The number of speakers in the talk\n",
    "    duration:         (int)     The duration of the talk in seconds\n",
    "    film_date:        (int)     The date of filming Unix timestamp\n",
    "    published_date:   (int)     The online publication Unix timestamp\n",
    "    comments:         (int)     The number of comments made on the talk\n",
    "    languages:        (int)     Number of languages available for talk\n",
    "    ratings:          (dict)    The various ratings given to the talk\n",
    "    url:              (url)     The URL of the talk\n",
    "    views:            (int)     The number of views on the talk\n",
    "    related_talks:    (dict)    List of dict of 6 related talks\n",
    "    tags:             (list)    The themes associated with the talk\n",
    "    \n",
    "We will describe this data more thoroughly as we develop the visualizations, but we would like to highlight some information that helps us understand this data, and with that, some interesting facts worth mentioning. The most common speaker occupation is \"Writer\" with 45 occurrences, followed by \"Designer\" with a total of 34 occurrences. The total number of occupations among speakers is 1458. The average talk is 13.7 minutes long, with the shortest being 2.25 minutes and the longest being about 1.5 hours, which was given by the author of \"The Hitchhiker's Guide to the Galaxy\". The average TED talk has been translated to 27 languages, and there are 86 talks that have no assigned language. These talks have a mean number of views significantly lower than average, and they are mainly musical presentations. The average number of views is 1.6 million and the talk with highest number of views has been seen almost 50 million times and it's called \"Do schools kill creativity?\".\n",
    "\n",
    "An important variable we must understand is the one named \"ratings\". This variable is a categorical description of the talk. It is opinion-based and given by the online audience: after watching the video, the viewer is asked: *\"How would you describe this talk? Tell us by choosing up to three words. (If you choose just one, it will count three times.)\".*  Afterwards, the viewer is given 14 possible adjectives to describe the talk with, and can only choose 3 of them. The same 14 possibilities are given to all viewers.\n",
    "\n",
    "With regards to the video tags, TED gives each video a number of possible tags to link a talk with different topics. The average video has 7.56 tags, with some videos having over 30 tags and some having just one. The other important variable that will be useful for us to observe is the related talks. Every video is given a connection to 6 other videos that are suggested for the viewer to watch. We suggest to improve these two metrics, having specific tags that might be more useful for the viewer, and testing if there is any link in the related talks with the number of views the talks have.\n",
    "\n",
    "### Transcript Dataset\n",
    "The transcript dataset contains the transcripts for 2467 TED talks. In this database we found three duplicates. We decided to analyze only the talks that are found on both databases in order to have homogeneous data. The 86 talks for which there is no transcript data are the most recent ones. Therefore, we discarded the data that was duplicated and for which there were no transcripts.\n",
    "\n",
    "##### Source:\n",
    "The data has been scraped from the official TED Website and is available under\n",
    "the Creative Commons License. It was retrieved from the Kaggle featured data sets in October 2017.\n",
    "\n",
    "## Data Preparation\n",
    "### Data Munging\n",
    "\n",
    "The two dataset were merged in order to consolidate all of the information. A RegEx was implemented in order to formally count the number of words in each talk, given that the information contained was in a string with str symbols (i.e. contained symbols representing newlines, tabs, etc.). The duration of the videos was changed to minutes to enable a words per minute (WPM) analysis of the talks. This will allow to do analysis of the pace of each talk.\n",
    "\n",
    "Most of the data represented originally had lists and dictionaries structure. When they were scrapped they were all saved as strings. A function 'g' was created in order to convert the strings back to their original data structures (dictionaries and lists). In the case of the urls, the last part of the string was erased in order to have functional links. All dates contained in the datasets were changed from Unix timestamps to \"datetime\" objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clean the URL's to make them useful\n",
    "mainData['url'] = mainData['url'].apply(lambda x: x[:-1])\n",
    "transcripts['url'] = transcripts['url'].apply(lambda x: x[:-1])\n",
    "mainData.index = mainData['url']\n",
    "transcripts.index = transcripts['url']\n",
    "\n",
    "# Drop any duplicates\n",
    "mainData.drop_duplicates('url',inplace=True)\n",
    "transcripts.drop_duplicates('url',inplace=True)\n",
    "\n",
    "# take away the colums repeated by the index\n",
    "mainData = mainData.drop('url',axis=1)\n",
    "transcripts = transcripts.drop('url',axis=1)\n",
    "\n",
    "# We concatenate the two datasets with an inner join\n",
    "mainData = pd.concat([mainData, transcripts], axis=1, join='inner')\n",
    "\n",
    "# Format the date variable\n",
    "def change_t(timestamp):\n",
    "    return datetime.utcfromtimestamp(timestamp)\n",
    "mainData[\"film_date\"] = mainData[\"film_date\"].apply(change_t)\n",
    "mainData[\"published_date\"] = mainData[\"published_date\"].apply(change_t)\n",
    "\n",
    "# Convert the data from strings into their original data structure\n",
    "g = lambda x: eval(x, {'__builtins__':None}, {})\n",
    "mainData['ratings'] = mainData['ratings'].apply(g)\n",
    "mainData['tags'] = mainData['tags'].apply(g)\n",
    "mainData['related_talks'] = mainData['related_talks'].apply(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Engineering\n",
    "\n",
    "One of the questions that we desire to answer is how the audience receives a talk. For this reason we have classified the possible categorical ratings that the audience can give to a talk as positive or negative ratings. \n",
    "\n",
    "- Positives = ['Funny', 'Beautiful', 'Ingenious', 'Courageous', 'Informative', 'Fascinating', 'Persuasive', 'Jaw-dropping', 'Inspiring']\n",
    "\n",
    "- Negatives = ['Obnoxious', 'OK', 'Confusing', 'Unconvincing', 'Longwinded']\n",
    "\n",
    "In order to compare the ratings for different talks we added a variable that gives the ratio of total number of negative ratings over the total number of ratings given for each individual talk. From here we derived the positive ratio.\n",
    "\n",
    "We also created a column regarding the total number of ratings, given that this can give some insight on how impactful the Talk was, either positively or negatively, given that comments are done when Talks are skewed to a positive or negative side of the spectrum.\n",
    "\n",
    "A variable called WPM was created that corresponds to the number of words per minute (i.e. pace of the speech). Another variable that was created was \"original ted\" which marks the talks that are given in an original TED conference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add a variable for word counts\n",
    "f = lambda x: len(re.findall(r'\\w+', x))\n",
    "mainData['word_count'] = mainData['transcript'].apply(f)\n",
    "Duration_Minutes = mainData['duration'].apply(lambda x: x/60)\n",
    "mainData.insert(3,'Duration_Minutes',Duration_Minutes)\n",
    "mainData['wpm'] = mainData['word_count']/mainData['Duration_Minutes']\n",
    "\n",
    "# Categorize the ratings with a binary variable\n",
    "def neg_ratings(list_dict):\n",
    "    negative = ['Obnoxious','OK','Confusing','Unconvincing','Longwinded']\n",
    "    neg_count = 0\n",
    "    pos_count = 0\n",
    "    for i in list_dict:\n",
    "        if i['name'] in negative:\n",
    "            neg_count += i['count']\n",
    "        else:\n",
    "            pos_count += i['count']\n",
    "    return(neg_count/(neg_count+pos_count))\n",
    "def totalcount(list_dict):\n",
    "    total = 0\n",
    "    for i in list_dict:\n",
    "        total += i['count']\n",
    "    return(total)\n",
    "mainData['Tots_Ratings'] = mainData['ratings'].apply(totalcount)\n",
    "mainData['Neg_Ratio'] = mainData['ratings'].apply(neg_ratings)\n",
    "mainData['Pos_Ratio'] = 1 - mainData['Neg_Ratio']\n",
    "# Binary variable to determine if it is original TED or other (e.g. TEDx)\n",
    "def original_ted(x):\n",
    "    myre = '^TED[0-9]{4}$'\n",
    "    if re.search(myre,x):\n",
    "        return(1)\n",
    "    else:\n",
    "        return(0)\n",
    "mainData['Real Ted'] = mainData[\"event\"].apply(original_ted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Appendix A) Proof that categorizing and regressing are unprofitable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create categorical variable\n",
    "describe_view = mainData[\"views\"].describe()\n",
    "views = pd.cut(mainData[\"views\"],[describe_view[\"min\"]-1,\n",
    "            describe_view[\"25%\"], describe_view[\"50%\"], \n",
    "            describe_view[\"75%\"], describe_view[\"max\"]+1],\n",
    "            labels=[\"0\", \"1\", \"2\", \"3\"])\n",
    "mainData[\"cut_views\"] = views\n",
    "Independent = mainData[['Duration_Minutes','num_speaker','word_count', 'wpm', 'Neg_Ratio', 'Pos_Ratio']]\n",
    "Target1 = mainData.cut_views\n",
    "Target2 = mainData.views\n",
    "Independent = StandardScaler().fit_transform(Independent)\n",
    "Independent2 = np.copy(Independent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=-1, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=10, scale_pos_weight=1, seed=None,\n",
       "       silent=False, subsample=1),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'learning_rate': [0.1, 0.2, 0.4, 0.5, 0.6, 0.7], 'gamma': [0, 5, 100], 'reg_lambda': [100, 50, 10, 1, 0.5]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='neg_mean_absolute_error', verbose=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TED_Regress2 = XGBRegressor(silent=False, n_jobs=-1,reg_lambda=10,gamma=0,learning_rate=.1)\n",
    "# new_score = -np.mean(cross_val_score(TED_Regress,Independent, Target, n_jobs=-1, verbose=1, scoring='neg_mean_absolute_error'))\n",
    "parameters = {'learning_rate':[.1, .2, .4, .5, .6, .7], 'gamma':[0, 5, 100], 'reg_lambda':[100, 50, 10, 1, .5]}\n",
    "GCV2 = GridSearchCV(TED_Regress2, parameters,scoring='neg_mean_absolute_error',n_jobs=-1)\n",
    "GCV2.fit(Independent,Target2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TED_Regress1 = XGBClassifier(silent=False, n_jobs=-1,reg_lambda=10,gamma=0,learning_rate=.1)\n",
    "# new_score = -np.mean(cross_val_score(TED_Regress,Independent, Target, n_jobs=-1, verbose=1, scoring='neg_mean_absolute_error'))\n",
    "parameters = {'learning_rate':[.1, .2, .4, .5, .6, .7], 'gamma':[0, 5, 100], 'reg_lambda':[100, 50, 10, 1, .5]}\n",
    "GCV1 = GridSearchCV(TED_Regress1, parameters, scoring=None, n_jobs=-1)\n",
    "GCV1.fit(Independent2,Target1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(GCV1.best_estimator_)\n",
    "print('Best Index         ',GCV1.best_index_)\n",
    "print('Best Parameters    ',GCV1.best_params_)\n",
    "print('Accuracy           ',GCV1.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(GCV2.best_estimator_)\n",
    "print('Best Index         ',GCV2.best_index_)\n",
    "print('Best Parameters    ',GCV2.best_params_)\n",
    "print('Best Mean Abs Score',-GCV2.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wikipedia_searcher(word, stop_words, num_searched=21):\n",
    "    \"\"\"\n",
    "    Input: a word, phrase, or otherwise anything that can be searched on Wikipedia.\n",
    "    \n",
    "    CAUTION: If you search a word that requires a disambiguation through Wikipedia \n",
    "    (e.g., cell: jail cell, biology cell, storm cell, etc), this function will throw an error.\n",
    "    \n",
    "    Returns: A list of up to 5 words closely related to the input word.\n",
    "    \n",
    "    This function works by searching the phrase on Wikipedia, then returning the 5 most common\n",
    "    words from the summary at the top. Summaries tend to explain what the word means, using\n",
    "    other words closely related to the input word, so this function works very well.\n",
    "    I haven't checked the code too closely for optimizing speed, the 11,000-word dataset is not\n",
    "    overly prohibitive yet.\n",
    "\n",
    "    We pass in the word to Wikipedia's summary.\n",
    "    If it doesn't work, we'll go to GloVe to get the most commonly\n",
    "    associated words to the ambiguous words.\n",
    "    BUT if all that fails, then we'll just pick one of the disambiguations\n",
    "    at random to use as our word.\n",
    "    Really, the word doesn't specify which disambiguation it is referring to,\n",
    "    so this is our best remaining option.\n",
    "    \"\"\"\n",
    "    if pd.isnull(word):\n",
    "        return []\n",
    "    try:\n",
    "        summary = wikipedia.summary(word)\n",
    "        definite_word = word\n",
    "    except wikipedia.exceptions.DisambiguationError as e:\n",
    "        for desperate_word in e.options:\n",
    "            try:\n",
    "                summary = wikipedia.summary(desperate_word)\n",
    "                definite_word = desperate_word\n",
    "                break\n",
    "            except:\n",
    "                continue\n",
    "    print(definite_word)\n",
    "    summary = re.sub(r'[^\\w\\s]','',summary).replace('\\n',' ').rstrip().lower().split(' ')\n",
    "    summary = [word for word in summary if word not in stop_words]\n",
    "    mydict = Counter(summary)\n",
    "    return [i[0] for i in mydict.most_common(num_searched)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = codecs.open('stop_words.txt').readlines()\n",
    "stopwords = [word[:-1] for word in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wiki_dicts = []\n",
    "for j in A21:\n",
    "    all_words = []\n",
    "    for i in j:\n",
    "        print(i)\n",
    "        all_words += wikipedia_searcher(i,stopwords)\n",
    "    all_dicts.append(Counter(all_words))\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('Wikipedia_dict.pickle', 'wb') as f:\n",
    "    pickle.dump(wiki_dicts, f)  \n",
    "\n",
    "with open('Wikipedia_dict.pickle', 'rb') as f:\n",
    "    wiki_dicts = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j=0\n",
    "for i in all_dicts:\n",
    "    print(A21[j]);j+=1\n",
    "    print(*[i.most_common()[j][0] for j in range(5)],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*A21,sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/acmeshare/juan9310/TedProject'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "325it [00:00, 3248.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['glove.txt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:54, 7308.94it/s]\n"
     ]
    }
   ],
   "source": [
    "datadict = {}\n",
    "FILES = glob.glob('glove*.txt')\n",
    "print(FILES)\n",
    "file = FILES[0]\n",
    "with open(file) as f:\n",
    "    for ind, l in tqdm(enumerate(f)):\n",
    "        line = l.split(' ')\n",
    "        word, vec = line[0], np.array(line[1:]).astype(np.float32)\n",
    "        datadict[word] = vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove_words = np.array(list(datadict.keys()))\n",
    "glove_A = np.array(list(datadict.values()))\n",
    "\n",
    "#By experimentation, keeping 27 dimensions keeps 70% of the variance, which should be ok.\n",
    "pca = PCA(n_components=30) \n",
    "PCA_A = pca.fit_transform(glove_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 300)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 30)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PCA_A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_tree = KDTree(glove_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def glove_searcher(word, num_searched = 21):\n",
    "    if pd.isnull(word):\n",
    "        return []\n",
    "    word_itself = [word]\n",
    "    word_itself.extend(word.split(' '))\n",
    "    word_itself.extend([i+'s' for i in word_itself]) #Add plurals too; we don't want those.\n",
    "    \n",
    "    word_vector = glove_A[glove_words==word][0] #accessing the 0th entry is necessary.\n",
    "    dists, vec_locs = glove_tree.query(word_vector, 2*num_searched+1) #There may be up to 2 words in here from the input word.\n",
    "    associated_words = glove_words[vec_locs]\n",
    "    truncated = [i for i in associated_words if i not in word_itself]\n",
    "    \n",
    "    return np.array([i for i in truncated if i not in stopwords])[:num_searched] #Up to 5 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['children', 'parents', 'mother', 'infant', 'girl', 'daughter',\n",
       "       'boy', 'newborn', 'instance', 'father', 'baby', 'kids', 'victim',\n",
       "       'siblings', 'birth', 'teenage', 'couple', 'young', 'daughters',\n",
       "       'teenagers', 'husband'], dtype='<U10')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'child'\n",
    "glove_searcher(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_dicts = []\n",
    "for j in A21:\n",
    "    all_words = []\n",
    "    for i in j:\n",
    "        print(i)\n",
    "        all_words += list(glove_searcher(i))\n",
    "    glove_dicts.append(Counter(all_words))\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Glove_dict.pickle', 'wb') as f:\n",
    "    pickle.dump(glove_dicts, f)  \n",
    "\n",
    "with open('Glove_dict.pickle', 'rb') as f:\n",
    "    Glove_dicts = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
