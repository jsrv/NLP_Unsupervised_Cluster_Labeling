{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords as stp_wrd\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import collections\n",
    "import re\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from scipy.special import gammaln\n",
    "import math\n",
    "from math import sqrt\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import invgamma\n",
    "import time as tm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Data\n",
    "mainData = pd.read_csv('mainData.csv')\n",
    "all_videos = pd.read_csv('all_videos.csv')\n",
    "# Load the transcripts for each DF\n",
    "Transcripts_m = mainData.transcript\n",
    "Transcripts_a = all_videos.transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('stop_words.txt', 'r') as stopfile:\n",
    "    stops = stopfile.read().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Output = list(Transcripts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA with Gibbs Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LDACGS:\n",
    "    \"\"\"Do LDA with Gibbs Sampling.\"\"\"\n",
    "\n",
    "    def __init__(self, n_topics, alpha=0.1, beta=0.1):\n",
    "        \"\"\"Initialize system parameters.\"\"\"\n",
    "        self.n_topics = n_topics\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "    def buildCorpus(self, filename, stopwords_file='stop_words.txt'):\n",
    "        \"\"\"Read the given filename and build the vocabulary.\"\"\"\n",
    "        # timed = tm.time()\n",
    "        with open(filename, 'r') as infile:\n",
    "            doclines = [re.sub(r'[^\\w\\s]','',line).rstrip().lower().split(' ') for line in infile]\n",
    "        n_docs = len(doclines)\n",
    "        self.vocab = list({v for doc in doclines for v in doc})\n",
    "        if stopwords_file:\n",
    "            with open(stopwords_file, 'r') as stopfile:\n",
    "                stops = stopfile.read().split()\n",
    "            self.vocab = [x for x in self.vocab if x not in stops]\n",
    "            self.vocab.sort()\n",
    "        self.documents = []\n",
    "        for i in tqdm(range(n_docs)):\n",
    "            self.documents.append({})\n",
    "            for j in range(len(doclines[i])):\n",
    "                if doclines[i][j] in self.vocab:\n",
    "                    self.documents[i][j] = self.vocab.index(doclines[i][j])\n",
    "         #print('Time building: ',tm.time()-timed)\n",
    "\n",
    "    def initialize(self):\n",
    "        \"\"\"Initialize the three count matrices.\"\"\"\n",
    "        self.n_words = len(self.vocab)\n",
    "        self.n_docs = len(self.documents)\n",
    "        \n",
    "        # Initialize the three count matrices.\n",
    "        # The (i,j) entry of self.nmz is the number of words in document i assigned to topic j.\n",
    "        self.nmz = np.zeros((self.n_docs, self.n_topics))\n",
    "        # The (i,j) entry of self.nzw is the number of times term j is assigned to topic i.\n",
    "        self.nzw = np.zeros((self.n_topics, self.n_words))\n",
    "        # The (i)-th entry is the number of times topic i is assigned in the corpus.\n",
    "        self.nz = np.zeros(self.n_topics)\n",
    "        # Initialize the topic assignment dictionary.\n",
    "        self.topics = {} # key-value pairs of form (m,i):z\n",
    "\n",
    "        for m in range(self.n_docs):\n",
    "            for i in self.documents[m]:\n",
    "                # Get random topic assignment, i.e. z = ...\n",
    "                # Increment count matrices\n",
    "                # Store topic assignment, i.e. self.topics[(m,i)]=z\n",
    "                w = self.documents[m][i]\n",
    "                z = np.random.randint(0,self.n_topics,1)\n",
    "                self.nmz[m,z] += 1 \n",
    "                self.nzw[z,w] += 1\n",
    "                self.nz[z] += 1\n",
    "                self.topics[(m,i)] = z\n",
    "                \n",
    "    def sample(self, filename, burnin=100, sample_rate=10, n_samples=10, stopwords_file='stop_words.txt'):\n",
    "        self.buildCorpus(filename, stopwords_file)\n",
    "        self.initialize()\n",
    "        self.total_nzw = np.zeros((self.n_topics, self.n_words))\n",
    "        self.total_nmz = np.zeros((self.n_docs, self.n_topics))\n",
    "        self.logprobs = np.zeros(burnin + sample_rate*n_samples)\n",
    "        j=0\n",
    "        for i in tqdm(range(burnin)):\n",
    "            # Sweep and store log likelihood.\n",
    "            self._sweep()\n",
    "            self.logprobs[j] = self._loglikelihood()\n",
    "            j+=1\n",
    "        for i in tqdm(range(n_samples*sample_rate)):\n",
    "            # Sweep and store log likelihood\n",
    "            self._sweep()\n",
    "            self.logprobs[j] = self._loglikelihood()\n",
    "            j+=1\n",
    "            if not i % sample_rate:\n",
    "                # accumulate counts\n",
    "                self.total_nzw += self.nzw\n",
    "                self.total_nmz += self.nmz\n",
    "\n",
    "    def phi(self):\n",
    "        phi = self.total_nzw + self.beta\n",
    "        self._phi = phi / np.sum(phi, axis=1)[:,np.newaxis]\n",
    "\n",
    "    def theta(self):\n",
    "        theta = self.total_nmz + self.alpha\n",
    "        self._theta = theta / np.sum(theta, axis=1)[:,np.newaxis]\n",
    "\n",
    "    def topterms(self,n_terms=10):\n",
    "        self.phi()\n",
    "        self.theta()\n",
    "        vec = np.atleast_2d(np.arange(0,self.n_words))\n",
    "        topics = []\n",
    "        for k in range(self.n_topics):\n",
    "            probs = np.atleast_2d(self._phi[k,:])\n",
    "            mat = np.append(probs,vec,0)\n",
    "            sind = np.array([mat[:,i] for i in np.argsort(mat[0])]).T\n",
    "            topics.append([self.vocab[int(sind[1,self.n_words - 1 - i])] for i in range(n_terms)])\n",
    "        return topics\n",
    "\n",
    "    def toplines(self,n_lines=5):\n",
    "        lines = np.zeros((self.n_topics,n_lines))\n",
    "        for i in range(self.n_topics):\n",
    "            args = np.argsort(self._theta[:,i]).tolist()\n",
    "            args.reverse()\n",
    "            lines[i,:] = np.array(args)[0:n_lines] + 1\n",
    "        return lines\n",
    "\n",
    "    def _removeStopwords(self, stopwords):\n",
    "        return [x for x in self.vocab if x not in stopwords]\n",
    "\n",
    "    def _conditional(self, m, w):\n",
    "        dist = (self.nmz[m,:] + self.alpha) * (self.nzw[:,w] + self.beta) / (self.nz + self.beta*self.n_words)\n",
    "        return dist / np.sum(dist)\n",
    "\n",
    "    def _sweep(self):\n",
    "        for m in range(self.n_docs):\n",
    "            for i in self.documents[m]:\n",
    "                # Retrieve vocab index for i-th word in document m.\n",
    "                w = self.documents[m][i]\n",
    "                # Retrieve topic assignment for i-th word in document m.\n",
    "                z = self.topics[(m,i)]\n",
    "                # Decrement count matrices.\n",
    "                self.nmz[m,z] -= 1 \n",
    "                self.nzw[z,w] -= 1\n",
    "                self.nz[z] -= 1\n",
    "                self.topics[(m,i)] = z\n",
    "                # Get conditional distribution.\n",
    "                theta = self._conditional(m,w)\n",
    "                # Sample new topic assignment.\n",
    "                z_ = np.argmax(np.random.multinomial(1,theta))\n",
    "                # Increment count matrices.\n",
    "                self.nmz[m,z_] += 1 \n",
    "                self.nzw[z_,w] += 1\n",
    "                self.nz[z_] += 1\n",
    "                # Store new topic assignment.\n",
    "                self.topics[(m,i)] = z_\n",
    "\n",
    "    def _loglikelihood(self):\n",
    "        lik = 0\n",
    "        for z in range(self.n_topics):\n",
    "            lik += np.sum(gammaln(self.nzw[z,:] + self.beta)) - gammaln(np.sum(self.nzw[z,:] + self.beta))\n",
    "            lik -= self.n_words * gammaln(self.beta) - gammaln(self.n_words*self.beta)\n",
    "\n",
    "        for m in range(self.n_docs):\n",
    "            lik += np.sum(gammaln(self.nmz[m,:] + self.alpha)) - gammaln(np.sum(self.nmz[m,:] + self.alpha))\n",
    "            lik -= self.n_topics * gammaln(self.alpha) - gammaln(self.n_topics*self.alpha)\n",
    "        return lik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TED_NLP = LDACGS(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2017/2017 [9:06:09<00:00, 12.05s/it]  \n",
      "100%|██████████| 100/100 [1:01:56<00:00, 36.93s/it]\n",
      "100%|██████████| 100/100 [1:01:43<00:00, 37.01s/it]\n"
     ]
    }
   ],
   "source": [
    "TED_NLP.sample('Transcripts.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'spinal', 'cord', 'arm', 'gabby', 'limb', 'chuck', 'bionic', 'ebola', 'nobel']\n",
      "['city', 'cities', 'car', 'cars', 'city,', 'urban', 'city.', 'york', 'cities,', 'public']\n",
      "['war', 'military', 'security', 'violence', 'war,', 'war.', 'nuclear', 'weapons', 'police', 'peace']\n",
      "['♫♫', 'political', 'democracy', 'religion', 'la', 'gay', 'religious', 'election', 'twitter', 'politics']\n",
      "['data', 'technology', 'information', 'computer', 'human', \"we're\", 'create', 'video', 'build', 'system']\n",
      "['legs', 'mr.', 'regret', 'abed', 'vulnerability', 'soap', 'comedy', 'athletes', 'record', 'storycorps']\n",
      "['energy', 'earth', 'universe', 'solar', 'space', 'light', 'planet', 'life', 'nuclear', 'billion']\n",
      "['dna', 'human', 'cells', 'species', 'genetic', 'animal', 'gene', 'animals', 'genes', 'cell']\n",
      "['brain', 'human', 'brain.', 'moral', 'study', 'brain,', 'brains', 'memory', 'compassion', 'social']\n",
      "['yorker', 'blah', 'glamour', 'dog', 'humor', 'fraud', 'glamorous', 'jealousy', 'cartoons', 'feynman']\n",
      "['♫', 'music', 'sound', 'play', 'song', 'hear', 'music,', 'it’s', 'music.', 'musical']\n",
      "['design', 'art', 'building', 'architecture', 'project', 'space', 'buildings', 'museum', 'designers', 'sort']\n",
      "['water', 'fish', 'ocean', 'sea', 'species', 'animals', 'forest', 'water.', 'water,', 'trees']\n",
      "['—', \"that's\", 'people', \"i'm\", \"we're\", 'make', \"there's\", 'things', \"they're\", 'kind']\n",
      "['food', 'eat', 'food.', 'food,', 'plant', 'bees', 'feed', 'kids', 'farmers', 'eating']\n",
      "['dictionary', 'givers', 'reputation', 'clock', 'climbers', 'mountain.', 'camp', 'dictionaries', 'twentysomethings', 'nr:']\n",
      "['health', 'cancer', 'medical', 'patients', 'blood', 'cells', 'disease', 'care', 'heart', 'patient']\n",
      "['people', 'said,', 'women', 'me,', 'life', 'time', 'men', 'young', 'love', 'years']\n",
      "['people', 'percent', 'world', '—', 'global', 'social', 'countries', 'money', 'million', 'make']\n",
      "['♪♪', 'poem', 'poetry', 'films', 'film', 'poems', 'vagina', 'paper', 'nigerian', '/']\n"
     ]
    }
   ],
   "source": [
    "print(*TED_NLP.topterms(),sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TED_NLP_50 = LDACGS(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2017/2017 [9:23:03<00:00, 12.36s/it]  \n",
      "100%|██████████| 300/300 [3:21:31<00:00, 39.61s/it]  \n",
      "100%|██████████| 150/150 [1:39:01<00:00, 39.51s/it]\n"
     ]
    }
   ],
   "source": [
    "TED_NLP_50.sample('Transcripts.txt',burnin=300,n_samples=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['women', 'men', 'sex', 'compassion', 'sexual', 'happiness', 'gender', 'gay', 'female', 'women.']\n",
      "['refugees', 'refugee', 'syrian', 'refugees,', 'syria', 'humanitarian', 'refugees.', 'doaa', 'asylum', 'bassem']\n",
      "['sacred', 'tapirs', 'redwood', 'climbers', 'clock', 'montana', 'peoples', 'mountain.', 'king', 'camp']\n",
      "['—', \"that's\", 'people', \"i'm\", \"we're\", 'make', 'things', \"there's\", \"they're\", 'kind']\n",
      "['water', 'energy', 'climate', 'earth', 'carbon', 'planet', 'oil', 'years', 'solar', 'ice']\n",
      "['cyrus', 'vulnerability', 'cylinder', 'adversity', 'hebrew', 'shame.', 'scrappers', 'cyrus,', 'vulnerability.', 'collagen']\n",
      "['dung', 'abed', 'beetles', 'beetle', 'dance.', 'lungi', 'dance,', 'honey', 'crash,', 'abed,']\n",
      "['poem', 'poetry', 'poems', 'laughter', '/', 'poem,', 'laughter,', 'poet', 'comedy', 'laughing']\n",
      "['stasi', 'fashion', 'copyright', 'protection.', 'justine', 'orgasm', 'protection', 'semen', \"justine's\", 'shaming']\n",
      "['password', 'passwords', 'passwords.', 'storycorps', 'passwords,', 'tony', 'psychopath', 'willful', 'bina', 'broadmoor']\n",
      "['', '♪♪', 'gabby', 'ebola', 'nobel', 's.o.s.', 'pete', 'ig', 'raisuddin', 'vaccines']\n",
      "['computer', 'data', 'robot', 'technology', 'machine', 'robots', 'computers', 'digital', 'machines', 'software']\n",
      "['tribes', 'stage', 'ushahidi', 'milo', 'carpet', 'tribe,', 'daycare', 'cuban', 'lolcats', 'nathan']\n",
      "['ems', 'hg:', 'em', 'member', 'tori', 'cards', 'deck', '4:', 'classroom.', 'audience']\n",
      "['aquatic', 'come,', 'lies', 'road', 'wait', 'swarm', 'closet', 'costume', 'armor', 'free,']\n",
      "['music', 'sound', 'play', 'hear', 'music,', 'music.', 'sounds', 'musical', '(music)', 'listening']\n",
      "['city', 'design', 'building', 'cities', 'city,', 'public', 'architecture', 'buildings', 'urban', 'city.']\n",
      "['disney', 'looting', 'paper', 'twentysomethings', 'brad', 'princess', 'emma', 'agile', 'benjamin', '20s']\n",
      "['war', 'political', 'military', 'democracy', 'religious', 'religion', 'civil', 'war,', 'muslim', 'arab']\n",
      "['silk', 'flag', 'spider', 'bridge', 'spiders', 'camel', 'flags', 'boy:', 'marcus:', 'nr:']\n",
      "['autism', 'autistic', 'cricket', 'foot', 'leg', 'hairs', 'cricket.', 'toes.', 'tuned', 'foot,']\n",
      "['said,', 'people', \"i'm\", 'me,', 'women', 'love', 'time', 'life', 'feel', 'me.']\n",
      "['dna', 'species', 'gene', 'genetic', 'genes', 'genome', 'bacteria', 'virus', 'animals', 'animal']\n",
      "['glamour', 'glamorous', 'introverts', 'glamorous.', 'electric', 'cluster', 'glamour.', 'gold', 'extroverts', 'introverted']\n",
      "['dinosaurs', 'dinosaur', 'mr.', 'skin', 'vitamin', 'dinosaurs.', 'dinosaurs,', 'mantis', 'teszler', 'nitric']\n",
      "['cancer', 'health', 'medical', 'cells', 'patients', 'blood', 'care', 'heart', 'disease', 'patient']\n",
      "['brain', 'brain,', 'brain.', 'neurons', 'brains', 'memory', 'cells', 'it’s', 'consciousness', 'neural']\n",
      "['language', 'english', 'words', 'language.', 'word', 'language,', 'english.', 'languages', 'dictionary', 'english,']\n",
      "['ants', 'ant', 'nest', 'colony', 'gang', 'workers', 'colonies', 'maintenance', 'ants,', 'interactions']\n",
      "['indus', 'yorker', 'humor', 'script', 'cartoons', 'rejection', 'cartoon', 'cartoons.', 'bf:', 'originals']\n",
      "['africa', 'african', 'africa.', 'africans', 'africa,', 'continent', 'nigeria', 'nigerian', 'ghana', 'nigeria.']\n",
      "['choice', 'choices', 'choice,', 'choices,', 'choose', 'puzzles', 'choosing', 'happiness', 'tea', 'choices.']\n",
      "['slime', 'hum', 'athletes', 'dog', 'mold', 'legs', 'wheelchair', 'nba', 'hey,', 'hum,']\n",
      "['foie', 'ok,', 'lie', 'lying', 'gras.', 'deception', 'geese', 'gras', 'undercover', 'ok.']\n",
      "['art', 'painting', 'artists', 'artist', 'arts', 'art.', 'museum', 'art,', 'paint', 'paintings']\n",
      "['food', 'eat', 'food.', 'food,', 'farmers', 'feed', 'eating', 'kids', 'farm', 'plant']\n",
      "['ab:', 'three-digit', 'number,', 'letting', '73', 'calculators', 'squared,', 'head-tail-head', 'toss', 'two-digit']\n",
      "['soap', 'howard', 'givers', 'spaghetti', 'television', 'lakota', 'tomato', 'sauce', 'mustard', 'handwashing']\n",
      "['universe', 'space', 'light', 'black', 'quantum', 'theory', 'stars', 'dark', 'planets', 'particles']\n",
      "['♫', 'song', 'song,', 'bonica', 'thula', 'loop', 'ah,', 'leopard', 'choir', 'intent.']\n",
      "['♫♫', 'la', 'li', 'song', 'regret', 'jealousy', \"'cause\", \"janitor's\", 'marries', 'da,']\n",
      "['film', 'films', 'film,', 'movie', 'fiction', 'viewer', 'film.', 'camera', 'man:', 'films,']\n",
      "['car', 'cars', 'car,', 'miles', 'fly', 'vehicle', 'flying', 'vehicles', 'car.', 'cars,']\n",
      "['game', 'games', 'video', 'microbes', 'game,', 'players', 'games,', 'game.', 'playing', 'games.']\n",
      "['internet', 'media', 'information', 'online', 'data', 'trust', 'digital', 'twitter', 'facebook', 'google']\n",
      "['people', 'percent', 'world', 'global', 'social', 'countries', 'money', 'country', 'united', 'dollars']\n",
      "['chinese', 'penguins', 'china', 'chop', 'fortune', 'oiled', 'penguin', 'penguins.', 'chinese,', 'chinese.']\n",
      "['black', 'police', 'justice', 'prison', 'law', 'legal', 'criminal', 'race', 'white', 'court']\n",
      "['low-power', 'cad', 'high-power', 'aortic', 'multipotentialite', 'coffeehouse', 'aorta.', 'nonverbals', 'bind.', 'multipotentialites']\n",
      "['fish', 'ocean', 'sea', 'animals', 'coral', 'marine', 'shark', 'ocean,', 'underwater', 'sharks']\n"
     ]
    }
   ],
   "source": [
    "print(*TED_NLP_50.topterms(),sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TED_NLP_20 = LDACGS(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2017/2017 [18:57:47<00:00, 22.20s/it]  \n",
      "100%|██████████| 1000/1000 [10:14:42<00:00, 36.03s/it] \n",
      "100%|██████████| 150/150 [1:30:12<00:00, 36.09s/it]\n"
     ]
    }
   ],
   "source": [
    "TED_NLP_20.sample('Transcripts.txt',burnin=1000,n_samples=15,stopwords_file='stop_words.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kids', 'school', 'students', 'children', 'education', 'game', 'teachers', 'learning', 'teach', 'games']\n",
      "['food', 'energy', 'water', 'climate', 'oil', 'carbon', 'percent', 'plant', 'solar', 'waste']\n",
      "['health', 'cancer', 'cells', 'medical', 'patients', 'blood', 'disease', 'dna', 'heart', 'care']\n",
      "['', 'war', 'police', 'military', 'violence', 'nuclear', 'law', 'criminal', 'weapons', 'prison']\n",
      "['moral', 'god', 'compassion', 'sex', 'religious', 'happiness', 'religion', 'human', 'sexual', 'choice']\n",
      "['water', 'species', 'animals', 'fish', 'ocean', 'sea', 'ice', 'years', 'forest', 'animal']\n",
      "['art', 'language', 'words', 'books', 'word', 'book', 'artist', 'painting', 'museum', 'artists']\n",
      "['music', 'sound', 'play', 'song', 'hear', 'music,', 'sounds', 'music.', 'musical', '(music)']\n",
      "['universe', 'space', 'earth', 'light', 'science', 'solar', 'planet', 'black', 'mars', 'quantum']\n",
      "['poem', 'poetry', 'i’m', 'don’t', 'you’re', 'that’s', 'poems', 'paper', 'smell', 'norden']\n",
      "['la', 'legs', 'dance', 'leg', 'athletes', 'foot', 'hum', 'feet', 'wheelchair', 'li']\n",
      "['brain', 'robot', 'brain,', 'brain.', 'robots', 'neurons', 'brains', 'memory', 'animal', 'visual']\n",
      "['mr.', 'abed', 'crash', 'wine', 'madison', 'seats', 'crash,', 'teszler', 'intangible', 'sylvia']\n",
      "['said,', 'women', 'people', 'me,', 'love', 'men', 'time', 'life', 'me.', 'told']\n",
      "['city', 'design', 'cities', 'building', 'city,', 'public', 'urban', 'architecture', 'buildings', 'built']\n",
      "['people', 'world', 'percent', 'global', 'countries', 'social', 'country', 'money', 'united', 'economic']\n",
      "['people', \"that's\", 'make', 'things', \"there's\", \"they're\", 'kind', 'time', 'lot', 'it.']\n",
      "['data', 'internet', 'information', 'computer', 'technology', 'digital', 'media', 'online', 'phone', 'machine']\n",
      "['fly', 'silk', 'flying', 'balloon', 'spider', 'airplane', 'flight', 'prize', 'glamour', 'aircraft']\n",
      "['film', 'ants', 'ant', 'films', 'film,', 'movie', 'nest', 'soap', 'colony', 'comedy']\n"
     ]
    }
   ],
   "source": [
    "print(*TED_NLP_20.topterms(),sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2017/2017 [10:39:48<00:00, 14.05s/it] \n",
      "100%|██████████| 1000/1000 [10:00:50<00:00, 35.94s/it]\n",
      "100%|██████████| 150/150 [1:29:58<00:00, 36.08s/it]\n"
     ]
    }
   ],
   "source": [
    "TED_NLP_21 = LDACGS(21)\n",
    "TED_NLP_21.sample('Transcripts.txt',burnin=1000,n_samples=15,stopwords_file='stop_words.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'universe', 'space', 'earth', 'solar', 'energy', 'light', 'black', 'mars', 'quantum']\n",
      "['data', 'internet', 'computer', 'information', 'digital', 'technology', 'media', 'online', 'phone', 'computers']\n",
      "['water', 'ocean', 'fish', 'climate', 'sea', 'species', 'carbon', 'ice', 'animals', 'earth']\n",
      "['dna', 'genetic', 'species', 'gene', 'genes', 'cells', 'bacteria', 'cell', 'animals', 'genome']\n",
      "['music', 'sound', 'play', 'language', 'song', 'hear', 'sounds', 'music,', 'music.', 'words']\n",
      "['sex', 'gay', 'sexual', 'romantic', 'sex,', 'marriage', 'sex.', 'couples', 'gang', 'vagina']\n",
      "['health', 'cancer', 'medical', 'patients', 'cells', 'blood', 'disease', 'care', 'heart', 'drug']\n",
      "['god', 'compassion', 'religious', 'religion', 'poem', 'religions', 'compassion,', 'god.', 'poetry', 'bible']\n",
      "['political', 'moral', 'democracy', 'election', 'vote', 'choice', 'politics', 'choices', 'trust', 'values']\n",
      "['kids', 'students', 'school', 'education', 'teachers', 'children', 'learning', 'teach', 'schools', 'teacher']\n",
      "['said,', 'people', 'women', 'me,', 'love', 'life', 'men', 'feel', 'time', 'told']\n",
      "['bridge', 'x-ray', 'feynman', 'twentysomethings', 'manuscript', 'career', 'intangible', 'imaging', 'internship', 'archie']\n",
      "['people', 'percent', 'world', 'global', 'countries', 'money', 'social', 'country', 'economic', 'dollars']\n",
      "['you’re', 'don’t', 'that’s', 'paper', 'hum', 'soap', 'we’re', 'they’re', 'hey,', 'crash']\n",
      "['people', 'make', 'things', 'kind', 'time', 'lot', 'it.', 'thing', 'years', 'back']\n",
      "['game', 'games', 'la', 'video', 'play', 'game,', 'players', 'ok,', 'playing', 'games,']\n",
      "['brain', 'brain,', 'brain.', 'neurons', 'brains', 'memory', 'visual', 'cells', 'consciousness', 'mental']\n",
      "['war', 'police', 'political', 'military', 'violence', 'law', 'civil', 'united', 'rights', 'security']\n",
      "['food', 'eat', 'plant', 'food,', 'food.', 'bees', 'waste', 'feed', 'ants', 'plants']\n",
      "['city', 'design', 'building', 'art', 'cities', 'public', 'space', 'city,', 'architecture', 'buildings']\n",
      "['robot', 'robots', 'fly', 'legs', 'ai', 'flying', 'robotic', 'artificial', 'machines', 'spinal']\n"
     ]
    }
   ],
   "source": [
    "print(*TED_NLP_21.topterms(),sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2017/2017 [10:52:34<00:00, 13.75s/it]  \n",
      "100%|██████████| 1000/1000 [9:56:10<00:00, 35.57s/it] \n",
      "100%|██████████| 150/150 [1:28:59<00:00, 35.56s/it]\n"
     ]
    }
   ],
   "source": [
    "TED_NLP_21 = LDACGS(21)\n",
    "TED_NLP_21.sample('Transcripts.txt',burnin=1000,n_samples=15,stopwords_file='stop_words.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bridge', 'tribes', 'storycorps', 'looting', 'click', 'bridge.', 'aquatic', 'panbanisha', 'swarm', 'ab:']\n",
      "['said,', 'women', 'people', 'me,', 'love', 'men', 'life', 'told', 'young', 'feel']\n",
      "['game', 'games', 'la', 'video', 'play', 'fly', 'game,', 'players', 'games,', 'flying']\n",
      "['war', 'military', 'nuclear', 'peace', 'war,', 'war.', 'refugees', 'weapons', 'security', 'refugee']\n",
      "['water', 'fish', 'ocean', 'sea', 'climate', 'ice', 'animals', 'species', 'carbon', 'forest']\n",
      "['people', 'percent', 'world', 'global', 'countries', 'money', 'economic', 'country', 'dollars', 'million']\n",
      "['dog', 'happiness', 'crash', 'car', 'golf', 'abed', 'psychology', 'crash,', 'seats', 'introverts']\n",
      "['music', 'art', 'play', 'sound', 'song', 'design', 'hear', 'music,', 'music.', 'artist']\n",
      "['political', 'people', 'social', 'government', 'rights', 'law', 'moral', 'public', 'media', 'democracy']\n",
      "['brain', 'brain,', 'brain.', 'neurons', 'brains', 'memory', 'visual', 'cells', 'animal', 'consciousness']\n",
      "['people', 'make', 'things', 'time', 'kind', 'lot', 'it.', 'thing', 'years', 'back']\n",
      "['universe', 'space', 'earth', 'light', 'solar', 'mars', 'black', 'energy', 'quantum', 'planet']\n",
      "['virus', 'hiv', 'malaria', 'flu', 'disease', 'vaccine', 'polio', 'infected', 'epidemic', 'vaccines']\n",
      "['god', 'language', 'compassion', 'religious', 'word', 'religion', 'words', 'book', 'english', 'books']\n",
      "['', 'sex', 'sexual', 'couples', 'sex.', 'sex,', 'disney', 'sexuality', 'a.i.', 'nobel']\n",
      "['cancer', 'health', 'medical', 'cells', 'patients', 'blood', 'care', 'heart', 'patient', 'drug']\n",
      "['food', 'dna', 'species', 'genetic', 'eat', 'bacteria', 'gene', 'genes', 'animals', 'plant']\n",
      "['city', 'energy', 'building', 'cities', 'design', 'water', 'car', 'cars', 'built', 'city,']\n",
      "['data', 'computer', 'technology', 'information', 'internet', 'digital', 'robot', 'machine', 'robots', 'computers']\n",
      "['kids', 'students', 'school', 'education', 'teachers', 'children', 'learning', 'teach', 'schools', 'teacher']\n",
      "['poem', 'silk', 'poetry', 'paper', 'spider', 'poems', 'hum', 'howard', 'ok,', '/']\n"
     ]
    }
   ],
   "source": [
    "print(*TED_NLP_21.topterms(),sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2017/2017 [8:05:05<00:00,  9.73s/it]  \n",
      "100%|██████████| 1000/1000 [9:21:00<00:00, 33.85s/it] \n",
      "100%|██████████| 150/150 [1:23:58<00:00, 33.86s/it]\n"
     ]
    }
   ],
   "source": [
    "TED_NLP_21a = LDACGS(21)\n",
    "TED_NLP_21a.sample('Transcripts.txt',burnin=1000,n_samples=15,stopwords_file='stop_words.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kids', 'school', 'children', 'students', 'education', 'teachers', 'learning', 'schools', 'child', 'teacher']\n",
      "['', 'people', 'time', 'life', 'years', 'day', 'back', 'world', 'thing', 'make']\n",
      "['music', 'play', 'sound', 'game', 'games', 'song', 'video', 'playing', 'hear', 'sounds']\n",
      "['water', 'energy', 'earth', 'planet', 'climate', 'carbon', 'years', 'oil', 'air', 'mars']\n",
      "['data', 'internet', 'information', 'online', 'media', 'phone', 'digital', 'people', 'google', 'web']\n",
      "['virus', 'hiv', 'disease', 'flu', 'malaria', 'vaccine', 'polio', 'infected', 'epidemic', 'vaccines']\n",
      "['people', 'human', 'social', 'life', 'love', 'good', 'happiness', 'compassion', 'feel', 'god']\n",
      "['language', 'books', 'laughter', 'english', 'book', 'words', 'word', 'read', 'stories', 'film']\n",
      "['ocean', 'fish', 'sea', 'animals', 'water', 'species', 'boat', 'coral', 'oceans', 'ice']\n",
      "['universe', 'light', 'space', 'stars', 'theory', 'physics', 'earth', 'planets', 'particles', 'black']\n",
      "['city', 'cities', 'car', 'cars', 'urban', 'street', 'york', 'public', 'map', 'streets']\n",
      "['bees', 'poem', 'poetry', 'bee', 'soap', 'poems', 'pollen', 'paper', 'flowers', 'flower']\n",
      "['people', 'world', 'percent', 'countries', 'money', 'country', 'years', 'dollars', 'africa', 'global']\n",
      "['cancer', 'health', 'patients', 'cells', 'disease', 'medical', 'blood', 'care', 'patient', 'heart']\n",
      "['brain', 'robot', 'robots', 'body', 'neurons', 'brains', 'memory', 'consciousness', 'cells', 'arm']\n",
      "['war', 'people', 'political', 'power', 'country', 'democracy', 'violence', 'states', 'police', 'rights']\n",
      "['dna', 'species', 'animals', 'human', 'genes', 'gene', 'bacteria', 'cells', 'cell', 'humans']\n",
      "['food', 'eat', 'farmers', 'plant', 'feed', 'bread', 'agriculture', 'eating', 'meat', 'farm']\n",
      "['design', 'art', 'building', 'architecture', 'project', 'space', 'buildings', 'designers', 'museum', 'made']\n",
      "['women', 'men', 'girls', 'woman', 'sex', 'black', 'gender', 'girl', 'gay', 'female']\n",
      "['', 'people', 'things', 'time', 'make', 'thing', 'world', 'years', 'kind', 'lot']\n"
     ]
    }
   ],
   "source": [
    "print(*TED_NLP_21a.topterms(),sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2017/2017 [7:36:59<00:00, 11.11s/it]  \n",
      "100%|██████████| 1000/1000 [9:12:05<00:00, 33.01s/it]  \n",
      "100%|██████████| 150/150 [1:24:59<00:00, 32.97s/it]\n"
     ]
    }
   ],
   "source": [
    "TED_NLP_21b = LDACGS(21)\n",
    "TED_NLP_21b.sample('Transcripts.txt',burnin=1000,n_samples=15,stopwords_file='stop_words.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['people', 'world', 'percent', 'countries', 'country', 'global', 'years', 'africa', 'change', 'social']\n",
      "['', 'people', 'time', 'things', 'make', 'world', 'thing', 'years', 'back', 'work']\n",
      "['women', 'men', 'life', 'people', 'family', 'woman', 'love', 'story', 'day', 'man']\n",
      "['city', 'design', 'art', 'building', 'cities', 'space', 'project', 'architecture', 'buildings', 'york']\n",
      "['game', 'play', 'games', 'video', 'playing', 'players', 'player', 'humor', 'comedy', 'yorker']\n",
      "['war', 'military', 'afghanistan', 'peace', 'violence', 'refugees', 'security', 'conflict', 'east', 'weapons']\n",
      "['data', 'internet', 'information', 'media', 'online', 'phone', 'digital', 'web', 'google', 'technology']\n",
      "['kids', 'school', 'children', 'students', 'education', 'teachers', 'learning', 'schools', 'child', 'teacher']\n",
      "['universe', 'earth', 'space', 'mars', 'light', 'planet', 'science', 'stars', 'planets', 'physics']\n",
      "['brain', 'computer', 'human', 'machine', 'technology', 'information', 'brains', 'data', 'memory', 'neurons']\n",
      "['laughter', 'flag', 'mr', 'script', 'prime', 'blah', 'writing', 'flags', 'paper', 'horse']\n",
      "['energy', 'water', 'car', 'oil', 'cars', 'nuclear', 'climate', 'power', 'percent', 'carbon']\n",
      "['cancer', 'health', 'disease', 'patients', 'medical', 'care', 'patient', 'blood', 'heart', 'doctors']\n",
      "['food', 'eat', 'plant', 'plants', 'bees', 'farmers', 'feed', 'agriculture', 'bread', 'eating']\n",
      "['music', 'sound', '', 'song', 'play', 'hear', 'sounds', 'la', 'voice', 'playing']\n",
      "['cells', 'dna', 'cell', 'human', 'species', 'genes', 'body', 'genetic', 'gene', 'bacteria']\n",
      "['money', 'company', 'business', 'trust', 'companies', 'market', 'buy', 'dollars', 'product', 'financial']\n",
      "['water', 'ocean', 'species', 'fish', 'animals', 'sea', 'ice', 'planet', 'years', 'forest']\n",
      "['arm', 'legs', 'poem', 'poetry', 'spinal', 'cord', 'body', 'wheelchair', 'disability', 'pain']\n",
      "['people', 'human', 'god', 'love', 'compassion', 'happiness', 'language', 'feel', 'moral', 'social']\n",
      "['robot', 'robots', 'fly', 'ants', 'flying', 'flight', 'ant', 'airplane', 'feet', 'colony']\n"
     ]
    }
   ],
   "source": [
    "print(*TED_NLP_21b.topterms(),sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2017/2017 [7:31:23<00:00, 10.29s/it]  \n",
      "100%|██████████| 1000/1000 [9:23:17<00:00, 33.37s/it] \n",
      "100%|██████████| 150/150 [1:24:13<00:00, 33.65s/it]\n"
     ]
    }
   ],
   "source": [
    "TED_NLP_21c = LDACGS(21)\n",
    "TED_NLP_21c.sample('Transcripts.txt',burnin=1000,n_samples=15,stopwords_file='stop_words.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['water', 'earth', 'ocean', 'energy', 'planet', 'sea', 'years', 'ice', 'oil', 'fish']\n",
      "['', 'people', 'time', 'life', 'day', 'back', 'years', 'love', 'laughter', 'story']\n",
      "['music', 'art', 'play', 'film', 'song', 'artist', 'artists', 'piece', 'la', 'painting']\n",
      "['food', 'water', 'forest', 'plants', 'trees', 'eat', 'plant', 'species', 'nature', 'waste']\n",
      "['women', 'men', 'girls', 'woman', 'sex', 'black', 'gender', 'gay', 'female', 'sexual']\n",
      "['universe', 'light', 'space', 'theory', 'science', 'stars', 'physics', 'planets', 'earth', 'particles']\n",
      "['kids', 'school', 'students', 'education', 'game', 'play', 'teachers', 'children', 'learning', 'games']\n",
      "['law', 'police', 'prison', 'justice', 'legal', 'crime', 'criminal', 'court', 'drug', 'violence']\n",
      "['god', 'compassion', 'happiness', 'moral', 'religion', 'human', 'religious', 'life', 'happy', 'love']\n",
      "['', 'people', 'things', 'time', 'make', 'world', 'years', 'thing', 'kind', 'lot']\n",
      "['robot', 'robots', 'fly', 'body', 'legs', 'ants', 'arm', 'ai', 'animal', 'video']\n",
      "['language', 'sound', 'words', 'voice', 'word', 'english', 'sounds', 'read', 'books', 'writing']\n",
      "['data', 'internet', 'computer', 'information', 'technology', 'phone', 'online', 'digital', 'computers', 'media']\n",
      "['cells', 'cancer', 'dna', 'cell', 'body', 'genes', 'gene', 'genome', 'bacteria', 'genetic']\n",
      "['animals', 'species', 'animal', 'humans', 'africa', 'virus', 'flu', 'dinosaurs', 'vaccine', 'disease']\n",
      "['city', 'design', 'cities', 'building', 'cars', 'car', 'buildings', 'space', 'architecture', 'built']\n",
      "['crash', 'soap', 'glamour', 'glamorous', 'simplicity', 'paper', 'norden', 'sauce', 'howard', 'abed']\n",
      "['trust', 'choice', 'choices', 'company', 'buy', 'money', 'business', 'market', 'organizations', 'brand']\n",
      "['people', 'war', 'political', 'world', 'country', 'power', 'government', 'democracy', 'states', 'united']\n",
      "['people', 'world', 'percent', 'countries', 'money', 'country', 'dollars', 'years', 'africa', 'year']\n",
      "['brain', 'health', 'patients', 'disease', 'medical', 'care', 'patient', 'heart', 'doctors', 'blood']\n"
     ]
    }
   ],
   "source": [
    "print(*TED_NLP_21c.topterms(),sep='\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
