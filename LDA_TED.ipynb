{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords as stp_wrd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import collections\n",
    "import re\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from scipy.special import gammaln\n",
    "import math\n",
    "from math import sqrt\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import invgamma\n",
    "import time as tm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops2 = \n",
    "# Load Data\n",
    "mainData = pd.read_csv('mainData.csv')\n",
    "all_videos = pd.read_csv('all_videos.csv')\n",
    "# Load the transcripts for each DF\n",
    "Transcripts_m = mainData.transcript\n",
    "Transcripts_a = all_videos.transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stop_words.txt', 'r') as stopfile:\n",
    "    stops = stopfile.read().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "Output = list(Transcripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3807"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoppers = open('stop_words.txt', 'w')\n",
    "stoppers.write(\"\\n\".join(stops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25909750"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoppers = open('Transcripts.txt', 'w')\n",
    "stoppers.write(\"\\n\".join(Output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA with Gibbs Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LDACGS:\n",
    "    \"\"\"Do LDA with Gibbs Sampling.\"\"\"\n",
    "\n",
    "    def __init__(self, n_topics, alpha=0.1, beta=0.1):\n",
    "        \"\"\"Initialize system parameters.\"\"\"\n",
    "        self.n_topics = n_topics\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "    def buildCorpus(self, filename, stopwords_file=None):\n",
    "        \"\"\"Read the given filename and build the vocabulary.\"\"\"\n",
    "        # timed = tm.time()\n",
    "        with open(filename, 'r') as infile:\n",
    "            doclines = [line.rstrip().lower().split(' ') for line in infile]\n",
    "        n_docs = len(doclines)\n",
    "        self.vocab = list({v for doc in doclines for v in doc})\n",
    "        if stopwords_file:\n",
    "            with open(stopwords_file, 'r') as stopfile:\n",
    "                stops = stopfile.read().split()\n",
    "            self.vocab = [x for x in self.vocab if x not in stops]\n",
    "            self.vocab.sort()\n",
    "        self.documents = []\n",
    "        for i in tqdm(range(n_docs)):\n",
    "            self.documents.append({})\n",
    "            for j in range(len(doclines[i])):\n",
    "                if doclines[i][j] in self.vocab:\n",
    "                    self.documents[i][j] = self.vocab.index(doclines[i][j])\n",
    "         #print('Time building: ',tm.time()-timed)\n",
    "\n",
    "    def initialize(self):\n",
    "        \"\"\"Initialize the three count matrices.\"\"\"\n",
    "        self.n_words = len(self.vocab)\n",
    "        self.n_docs = len(self.documents)\n",
    "        \n",
    "        # Initialize the three count matrices.\n",
    "        # The (i,j) entry of self.nmz is the number of words in document i assigned to topic j.\n",
    "        self.nmz = np.zeros((self.n_docs, self.n_topics))\n",
    "        # The (i,j) entry of self.nzw is the number of times term j is assigned to topic i.\n",
    "        self.nzw = np.zeros((self.n_topics, self.n_words))\n",
    "        # The (i)-th entry is the number of times topic i is assigned in the corpus.\n",
    "        self.nz = np.zeros(self.n_topics)\n",
    "        # Initialize the topic assignment dictionary.\n",
    "        self.topics = {} # key-value pairs of form (m,i):z\n",
    "\n",
    "        for m in range(self.n_docs):\n",
    "            for i in self.documents[m]:\n",
    "                # Get random topic assignment, i.e. z = ...\n",
    "                # Increment count matrices\n",
    "                # Store topic assignment, i.e. self.topics[(m,i)]=z\n",
    "                w = self.documents[m][i]\n",
    "                z = np.random.randint(0,self.n_topics,1)\n",
    "                self.nmz[m,z] += 1 \n",
    "                self.nzw[z,w] += 1\n",
    "                self.nz[z] += 1\n",
    "                self.topics[(m,i)] = z\n",
    "                \n",
    "    def sample(self, filename, burnin=100, sample_rate=10, n_samples=10, stopwords_file='stop_words.txt'):\n",
    "        self.buildCorpus(filename, stopwords_file)\n",
    "        self.initialize()\n",
    "        self.total_nzw = np.zeros((self.n_topics, self.n_words))\n",
    "        self.total_nmz = np.zeros((self.n_docs, self.n_topics))\n",
    "        self.logprobs = np.zeros(burnin + sample_rate*n_samples)\n",
    "        j=0\n",
    "        # timed = tm.time()\n",
    "        for i in tqdm(range(burnin)):\n",
    "            # Sweep and store log likelihood.\n",
    "            self._sweep()\n",
    "            self.logprobs[j] = self._loglikelihood()\n",
    "            j+=1\n",
    "        # print('Time burning: ',tm.time()-timed)\n",
    "        # timed = tm.time()\n",
    "        for i in tqdm(range(n_samples*sample_rate)):\n",
    "            # Sweep and store log likelihood\n",
    "            self._sweep()\n",
    "            self.logprobs[j] = self._loglikelihood()\n",
    "            j+=1\n",
    "            if not i % sample_rate:\n",
    "                # accumulate counts\n",
    "                self.total_nzw += self.nzw\n",
    "                self.total_nmz += self.nmz\n",
    "        #print('Time sampling: ',tm.time()-timed)\n",
    "                \n",
    "\n",
    "    def phi(self):\n",
    "        phi = self.total_nzw + self.beta\n",
    "        self._phi = phi / np.sum(phi, axis=1)[:,np.newaxis]\n",
    "\n",
    "    def theta(self):\n",
    "        theta = self.total_nmz + self.alpha\n",
    "        self._theta = theta / np.sum(theta, axis=1)[:,np.newaxis]\n",
    "\n",
    "    def topterms(self,n_terms=10):\n",
    "        self.phi()\n",
    "        self.theta()\n",
    "        vec = np.atleast_2d(np.arange(0,self.n_words))\n",
    "        topics = []\n",
    "        for k in range(self.n_topics):\n",
    "            probs = np.atleast_2d(self._phi[k,:])\n",
    "            mat = np.append(probs,vec,0)\n",
    "            sind = np.array([mat[:,i] for i in np.argsort(mat[0])]).T\n",
    "            topics.append([self.vocab[int(sind[1,self.n_words - 1 - i])] for i in range(n_terms)])\n",
    "        return topics\n",
    "\n",
    "    def toplines(self,n_lines=5):\n",
    "        lines = np.zeros((self.n_topics,n_lines))\n",
    "        for i in range(self.n_topics):\n",
    "            args = np.argsort(self._theta[:,i]).tolist()\n",
    "            args.reverse()\n",
    "            lines[i,:] = np.array(args)[0:n_lines] + 1\n",
    "        return lines\n",
    "\n",
    "    def _removeStopwords(self, stopwords):\n",
    "        return [x for x in self.vocab if x not in stopwords]\n",
    "\n",
    "    def _conditional(self, m, w):\n",
    "        dist = (self.nmz[m,:] + self.alpha) * (self.nzw[:,w] + self.beta) / (self.nz + self.beta*self.n_words)\n",
    "        return dist / np.sum(dist)\n",
    "\n",
    "    def _sweep(self):\n",
    "        for m in range(self.n_docs):\n",
    "            for i in self.documents[m]:\n",
    "                # Retrieve vocab index for i-th word in document m.\n",
    "                w = self.documents[m][i]\n",
    "                # Retrieve topic assignment for i-th word in document m.\n",
    "                z = self.topics[(m,i)]\n",
    "                # Decrement count matrices.\n",
    "                self.nmz[m,z] -= 1 \n",
    "                self.nzw[z,w] -= 1\n",
    "                self.nz[z] -= 1\n",
    "                self.topics[(m,i)] = z\n",
    "                # Get conditional distribution.\n",
    "                theta = self._conditional(m,w)\n",
    "                # Sample new topic assignment.\n",
    "                z_ = np.argmax(np.random.multinomial(1,theta))\n",
    "                # Increment count matrices.\n",
    "                self.nmz[m,z_] += 1 \n",
    "                self.nzw[z_,w] += 1\n",
    "                self.nz[z_] += 1\n",
    "                # Store new topic assignment.\n",
    "                self.topics[(m,i)] = z_\n",
    "\n",
    "    def _loglikelihood(self):\n",
    "        lik = 0\n",
    "        for z in range(self.n_topics):\n",
    "            lik += np.sum(gammaln(self.nzw[z,:] + self.beta)) - gammaln(np.sum(self.nzw[z,:] + self.beta))\n",
    "            lik -= self.n_words * gammaln(self.beta) - gammaln(self.n_words*self.beta)\n",
    "\n",
    "        for m in range(self.n_docs):\n",
    "            lik += np.sum(gammaln(self.nmz[m,:] + self.alpha)) - gammaln(np.sum(self.nmz[m,:] + self.alpha))\n",
    "            lik -= self.n_topics * gammaln(self.alpha) - gammaln(self.n_topics*self.alpha)\n",
    "        return lik"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
